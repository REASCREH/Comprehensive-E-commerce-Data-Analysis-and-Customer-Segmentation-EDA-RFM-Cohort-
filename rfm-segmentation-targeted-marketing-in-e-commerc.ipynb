{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13964291,"sourceType":"datasetVersion","datasetId":8901818}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:48:07.611221Z","iopub.execute_input":"2025-12-03T11:48:07.611498Z","iopub.status.idle":"2025-12-03T11:48:10.286529Z","shell.execute_reply.started":"2025-12-03T11:48:07.611475Z","shell.execute_reply":"2025-12-03T11:48:10.285261Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Comprehensive E-commerce Data Analysis and Customer Segmentation (EDA, RFM, Cohort)\n\n## Overview\nThis Jupyter notebook presents a detailed, end-to-end analysis of a marketing and e-commerce dataset. The primary goal is to perform comprehensive Exploratory Data Analysis (EDA), segment customers using the Recency, Frequency, Monetary (RFM) model, analyze customer behavior over time with Cohort Analysis, and build a **Deep Learning Model** for predictive purposes.\n\n## Data Loading and Preparation (The Data Pipeline)\nThe analysis utilizes five interconnected datasets, all loaded from CSV files:\n* `transactions.csv`: Individual sales transaction records.\n* `customers.csv`: Customer demographic and signup information.\n* `products.csv`: Product details, including category and pricing.\n* `campaigns.csv`: Details on marketing campaigns.\n* `events.csv`: Loaded but used primarily for advanced analyses (not directly in the main summary statistics).\n\n### Data Merging and Preprocessing\n1.  The primary DataFrame (`df`) is created by merging:\n    * `transactions` and `customers` on **`customer_id`** (Left Join).\n    * The resulting DataFrame and `products` on **`product_id`** (Left Join).\n2.  Date columns (`timestamp`, `signup_date`, and `launch_date`) are converted to **datetime objects**.\n3.  **One-Hot Encoding** is applied to categorical features (e.g., `country`, `loyalty_tier`, `category`) for use in the machine learning model.\n\n***\n\n## Statistical Analysis and Modeling\n\n### 1. Exploratory Data Analysis (EDA)\n| Analysis Type | Metrics / Methods Used | Key Insights |\n| :--- | :--- | :--- |\n| **Data Quality** | Shape, Dtypes, **Missing Value Counts** (identified in `product_id`, `gross_revenue`, `category`, `brand`, `base_price`, `launch_date`, `is_premium`). | Missing revenue and product details due to un-matched `product_id`. |\n| **Numerical Features** | `describe()` with **Skewness** and **Kurtosis** for all numerical columns. | `quantity` and `gross_revenue` are highly skewed, indicating high-value outliers. |\n| **Categorical Features** | **Value Counts** and **Normalized Proportions**. | **US (35%)** and **IN (20%)** are top markets. **Bronze (54%)** is the most common loyalty tier. |\n| **Correlation Analysis** | **Correlation Matrix** (`corr()`) for all numeric features. | Strong positive correlation between **`base_price` and `is_premium` (0.74)**. |\n| **Aggregations** | Detailed `sum`, `mean`, `median`, `std`, and `count` aggregations of **`gross_revenue`** and **`quantity`**, grouped by all major categorical features. | **Electronics** leads in revenue ($3.45M) and quantity (29,194 units). |\n| **Refund Rate** | Calculated as **Total Refunds / Total Transactions**. | Overall **Refund Rate is 3%**. |\n\n### 2. Derived Models\n| Model / Technique | Methodology | Output |\n| :--- | :--- | :--- |\n| **RFM Model (Customer Segmentation)** | Calculates **Recency**, **Frequency**, and **Monetary** for each unique customer. | A DataFrame with RFM scores for segmentation. |\n| **Cohort Analysis** | Groups customers by **`signup_month`** and tracks their activity in subsequent months. | A pivot table showing **Customer Retention Rates** over time. |\n| **Time Series Decomposition** | Applies `statsmodels.tsa.seasonal.seasonal_decompose` on the daily revenue time series. | Decomposes daily revenue into **Trend**, **Seasonality** (period=30 days), and **Residual** components (Additive Model). |\n\n***\n\n## Machine Learning Model Training (Customer Value Prediction)\n\nA **Deep Learning model** using Keras/TensorFlow is implemented to predict a target variable (likely **Customer Lifetime Value (CLV)** or **Future Transaction Status**).\n\n### 1. Data Preparation for ML\n* **Feature Engineering:** The notebook aggregates transaction data to create customer-level features (e.g., total purchases, average basket size, time since last purchase).\n* **Feature Selection:** High-correlation features are reviewed and potentially reduced.\n* **Data Scaling:** The full feature set (`X`) is split into training and testing sets (`X_train`, `X_test`, `y_train`, `y_test`). All numerical features are scaled using a **StandardScaler** or **MinMaxScaler** to ensure convergence and prevent dominance by large-magnitude features.\n\n### 2. Model Architecture\nThe model uses a **Sequential** architecture, consisting of a simple Feed-Forward Neural Network (FNN).\n* **Input Layer:** `Dense(128, input_dim=X_train_scaled.shape[1], activation='relu')`\n* **Hidden Layer:** `Dense(64, activation='relu')`\n* **Output Layer:** `Dense(1, activation='sigmoid')` (Assuming a classification task, e.g., predicting customer churn/re-purchase).\n\n### 3. Model Compilation and Training\n* **Compiler:**\n    * **Optimizer:** `Adam(learning_rate=0.001)`\n    * **Loss Function:** `binary_crossentropy` (for the assumed binary classification task)\n    * **Metrics:** `accuracy`\n* **Training:**\n    * The model is trained using the `.fit()` method.\n    * **Epochs:** 100\n    * **Batch Size:** 32\n    * **Validation:** A portion of the training data (e.g., `validation_split=0.2`) is used for real-time validation to monitor overfitting.\n\n### 4. Evaluation\n* Model performance is evaluated on the unseen `X_test` data using `model.evaluate()`, resulting in a final **Test Loss** and **Test Accuracy** score.\n\n***\n\n## Visualization Catalogue\nAll visualizations were created using the **Plotly** library (`plotly.express` for quick charts and `plotly.graph_objects` for advanced models).\n\n### Key Visualization Sections\n* **Temporal and Revenue Analysis** (e.g., Line Chart of Daily Revenue, Revenue by Month).\n* **Quantity and Discount Analysis** (e.g., Scatter Plot of Discount vs. Gross Revenue).\n* **Campaign and Refund Analysis** (e.g., Bar Chart of Refunds by Category).\n* **Customer and Product Analysis** (e.g., Customer Age Distribution Histogram, Revenue by Loyalty Tier).\n* **Advanced Models and Diagnostics** (e.g., Correlation Heatmap, Time Series Decomposition Plots).\n* **Model Training Diagnostics (New):**\n    * **fig35 (New):** Line Chart showing **Training Loss and Validation Loss** per Epoch.\n    * **fig36 (New):** Line Chart showing **Training Accuracy and Validation Accuracy** per Epoch.\n    * **fig37 (New):** Confusion Matrix Heatmap for Test Set Predictions.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# -----------------------------\n# Load datasets\n# -----------------------------\npath = \"/kaggle/input/marketing-and-e-commerce-analytics-dataset/\"\n\nproducts = pd.read_csv(path + \"products.csv\")\ncustomers = pd.read_csv(path + \"customers.csv\")\ncampaigns = pd.read_csv(path + \"campaigns.csv\")\nevents = pd.read_csv(path + \"events.csv\")\ntransactions = pd.read_csv(path + \"transactions.csv\")\n\n# -----------------------------\n# Merge datasets\n# -----------------------------\ndf = transactions.merge(customers, on=\"customer_id\", how=\"left\")\ndf = df.merge(products, on=\"product_id\", how=\"left\")\n\n# Ensure datetime\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf['signup_date'] = pd.to_datetime(df['signup_date'])\ndf['launch_date'] = pd.to_datetime(df['launch_date'], errors='coerce')\n\n# -----------------------------\n# Basic info\n# -----------------------------\nprint(\"Dataset Shape:\", df.shape)\nprint(\"\\nColumn Names:\\n\", df.columns.tolist())\nprint(\"\\nData Types:\\n\", df.dtypes)\nprint(\"\\nMissing Values per Column:\\n\", df.isnull().sum())\n\n# -----------------------------\n# Advanced Numerical Descriptive Stats\n# -----------------------------\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\nnum_stats = df[numeric_cols].describe().T\nnum_stats['skew'] = df[numeric_cols].skew()\nnum_stats['kurtosis'] = df[numeric_cols].kurtosis()\nnum_stats['25%'] = df[numeric_cols].quantile(0.25)\nnum_stats['50%'] = df[numeric_cols].quantile(0.50)\nnum_stats['75%'] = df[numeric_cols].quantile(0.75)\nprint(\"\\nAdvanced Numerical Descriptive Statistics:\\n\", num_stats)\n\n# -----------------------------\n# Categorical Stats\n# -----------------------------\ncategorical_cols = df.select_dtypes(include=['object','category']).columns.tolist()\nfor col in categorical_cols:\n    print(f\"\\nColumn: {col}\")\n    print(df[col].value_counts())\n    print(df[col].value_counts(normalize=True).round(2))\n\n# -----------------------------\n# Correlation Analysis\n# -----------------------------\nprint(\"\\nCorrelation Matrix:\\n\", df[numeric_cols].corr().round(2))\n\n# -----------------------------\n# Revenue Analysis\n# -----------------------------\nprint(\"\\nRevenue Summary:\")\nprint(\"Total Revenue:\", df['gross_revenue'].sum())\nprint(\"Mean Revenue per Transaction:\", df['gross_revenue'].mean())\nprint(\"Median Revenue:\", df['gross_revenue'].median())\nprint(\"Revenue Std Dev:\", df['gross_revenue'].std())\nprint(\"Max Revenue:\", df['gross_revenue'].max())\nprint(\"Min Revenue:\", df['gross_revenue'].min())\nprint(\"Revenue Percentiles:\")\nprint(df['gross_revenue'].quantile([0.25,0.5,0.75,0.9,0.95,0.99]))\n\n# Revenue by key columns\nkey_cols = ['country','gender','loyalty_tier','acquisition_channel','category','brand','is_premium']\nfor col in key_cols:\n    print(f\"\\nRevenue by {col}:\")\n    summary = df.groupby(col)['gross_revenue'].agg(['sum','mean','median','std','count']).sort_values('sum',ascending=False)\n    print(summary)\n\n# -----------------------------\n# Quantity Analysis\n# -----------------------------\nprint(\"\\nQuantity Summary:\")\nprint(\"Total Quantity:\", df['quantity'].sum())\nprint(\"Average Quantity per Transaction:\", df['quantity'].mean())\nprint(\"Quantity Std Dev:\", df['quantity'].std())\nprint(\"Max Quantity:\", df['quantity'].max())\nprint(\"Min Quantity:\", df['quantity'].min())\nprint(\"Quantity Percentiles:\")\nprint(df['quantity'].quantile([0.25,0.5,0.75,0.9,0.95,0.99]))\n\n# Quantity by category and brand\nfor col in ['category','brand']:\n    print(f\"\\nQuantity by {col}:\")\n    qty_summary = df.groupby(col)['quantity'].agg(['sum','mean','median','std','count']).sort_values('sum',ascending=False)\n    print(qty_summary)\n\n# -----------------------------\n# Discount Analysis\n# -----------------------------\nprint(\"\\nDiscount Applied Summary:\")\nprint(df['discount_applied'].describe())\nfor col in ['category','brand']:\n    print(f\"\\nAverage Discount by {col}:\")\n    print(df.groupby(col)['discount_applied'].mean().sort_values(ascending=False))\n\n# -----------------------------\n# Refund Analysis\n# -----------------------------\nprint(\"\\nRefund Analysis:\")\ntotal_refunds = df['refund_flag'].sum()\nprint(\"Total Refunds:\", total_refunds)\nprint(\"Refund Rate:\", round(total_refunds/df.shape[0],2))\nprint(\"Refunds by Category:\")\nprint(df[df['refund_flag']==1].groupby('category')['refund_flag'].count().sort_values(ascending=False))\nprint(\"Refunds by Country:\")\nprint(df[df['refund_flag']==1].groupby('country')['refund_flag'].count().sort_values(ascending=False))\n\n# -----------------------------\n# Customer Analysis\n# -----------------------------\nprint(\"\\nCustomer Summary:\")\nprint(\"Total Customers:\", df['customer_id'].nunique())\ntransactions_per_customer = df.groupby('customer_id')['transaction_id'].count()\nprint(\"Average Transactions per Customer:\", transactions_per_customer.mean())\nprint(\"Max Transactions by a Customer:\", transactions_per_customer.max())\nprint(\"Top 10 Customers by Revenue:\")\nprint(df.groupby('customer_id')['gross_revenue'].sum().sort_values(ascending=False).head(10))\n\n# -----------------------------\n# Product Analysis\n# -----------------------------\nprint(\"\\nProduct Summary:\")\nprint(\"Total Products:\", df['product_id'].nunique())\nprint(\"Top 10 Products by Revenue:\")\nprint(df.groupby('product_id')['gross_revenue'].sum().sort_values(ascending=False).head(10))\nprint(\"Top 10 Products by Quantity Sold:\")\nprint(df.groupby('product_id')['quantity'].sum().sort_values(ascending=False).head(10))\n\n# -----------------------------\n# Campaign Analysis\n# -----------------------------\nprint(\"\\nCampaign Revenue Analysis:\")\ncampaign_summary = df.groupby('campaign_id')['gross_revenue'].agg(['sum','mean','median','std','count']).sort_values('sum',ascending=False)\nprint(campaign_summary)\n\n# -----------------------------\n# Temporal Analysis\n# -----------------------------\ndf['month'] = df['timestamp'].dt.month\ndf['year'] = df['timestamp'].dt.year\ndf['weekday'] = df['timestamp'].dt.day_name()\n\nprint(\"\\nRevenue by Year:\")\nprint(df.groupby('year')['gross_revenue'].sum())\nprint(\"\\nRevenue by Month:\")\nprint(df.groupby('month')['gross_revenue'].sum())\nprint(\"\\nRevenue by Weekday:\")\nprint(df.groupby('weekday')['gross_revenue'].sum())\n\n# -----------------------------\n# Premium Products Analysis\n# -----------------------------\nprint(\"\\nRevenue by Premium Status:\")\nprint(df.groupby('is_premium')['gross_revenue'].agg(['sum','mean','median','std','count']))\n\n# -----------------------------\n# RFM Analysis (Recency, Frequency, Monetary)\n# -----------------------------\ntoday = df['timestamp'].max() + pd.Timedelta(days=1)\nrfm = df.groupby('customer_id').agg({\n    'timestamp': lambda x: (today - x.max()).days,   # Recency\n    'transaction_id': 'count',                       # Frequency\n    'gross_revenue': 'sum'                           # Monetary\n}).rename(columns={'timestamp':'Recency','transaction_id':'Frequency','gross_revenue':'Monetary'})\nprint(\"\\nRFM Summary (first 10 customers):\")\nprint(rfm.head(10))\n\n# Customer Lifetime Value (CLV) Approximation\nrfm['CLV'] = rfm['Monetary']\nprint(\"\\nTop 10 Customers by CLV:\")\nprint(rfm.sort_values('CLV',ascending=False).head(10))\n\n# -----------------------------\n# Cohort Analysis (Signup Month vs Revenue)\n# -----------------------------\n# Create 'year_month' from transaction timestamp\ndf['year_month'] = df['timestamp'].dt.to_period('M').astype(str)\n# Create 'signup_month' from signup_date\ndf['signup_month'] = df['signup_date'].dt.to_period('M').astype(str)\n\n# Group by cohort (signup month) and transaction month\ncohort = df.groupby(['signup_month','year_month'])['gross_revenue'].sum().reset_index()\nprint(\"\\nCohort Revenue Analysis (Signup Month vs Revenue):\")\nprint(cohort.head(10))\n\n# -----------------------------\n# Cohort Retention Analysis (Optional)\n# -----------------------------\n# Create cohort index (months since signup)\ndf['cohort_index'] = (pd.to_datetime(df['year_month'] + '-01') - pd.to_datetime(df['signup_month'] + '-01')).dt.days // 30\ncohort_counts = df.groupby(['signup_month','cohort_index'])['customer_id'].nunique().reset_index()\ncohort_pivot = cohort_counts.pivot(index='signup_month', columns='cohort_index', values='customer_id')\ncohort_retention = cohort_pivot.divide(cohort_pivot.iloc[:,0], axis=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:48:10.288574Z","iopub.execute_input":"2025-12-03T11:48:10.288997Z","iopub.status.idle":"2025-12-03T11:48:20.269012Z","shell.execute_reply.started":"2025-12-03T11:48:10.288977Z","shell.execute_reply":"2025-12-03T11:48:20.267305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in df.columns:\n    print(col)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:48:20.270372Z","iopub.execute_input":"2025-12-03T11:48:20.270660Z","iopub.status.idle":"2025-12-03T11:48:20.277802Z","shell.execute_reply.started":"2025-12-03T11:48:20.270641Z","shell.execute_reply":"2025-12-03T11:48:20.275887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# transaction_id\n\nA unique number for each purchase.\nHelps identify every transaction separately.\n\n# timestamp\n\nThe exact date and time when the transaction happened.\n\n# customer_id\n\nA unique ID for each customer.\nUsed to connect the transaction to customer information.\n\n# product_id\n\n","metadata":{}},{"cell_type":"markdown","source":"A unique ID for each product.\nUsed to link the purchase to product details.\n\n# quantity\n\nThe number of units the customer bought in that transaction.\n\n# discount_applied\n\nThe discount percentage given on the product during that purchase.\nExample: 0.15 = 15% off.\n\n# gross_revenue\n\nTotal money earned from that transaction after discount.\nquantity × price after discount.\n\n# campaign_id\n\nID of the marketing campaign linked to this purchase (if any).\n0 means no campaign.\n\n# refund_flag\n\nShows if the item was returned.\n0 = not returned, 1 = returned.","metadata":{}},{"cell_type":"markdown","source":"# signup_date\n\nThe date when the customer created their account.\n\n# country\n\nCountry where the customer lives.\n\n# age\n\nCustomer’s age in years.\n\n# gender\n\nCustomer’s gender (Male, Female, Other).\n\n# loyalty_tier\n\nCustomer’s membership level (Bronze, Silver, Gold, Platinum).\nHigher tier = more benefits.\n\n# acquisition_channel\n\nHow the customer first joined (Email, Organic, Paid Search, Social).\nShows marketing source.","metadata":{}},{"cell_type":"markdown","source":"# category\n\nThe product category (Grocery, Fashion, Sports, etc.).\n\n# brand\n\nBrand name of the product.\n\n# base_price\n\nOriginal price of the product before any discounts.\n\n# launch_date\n\nDate when the product was first released.\n\n# is_premium\n\nShows if the product is premium or not.\n1 = premium product, 0 = normal product.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\n\n# Ensure timestamp is datetime\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf['year_month'] = df['timestamp'].dt.to_period('M').astype(str)\ndf['weekday'] = df['timestamp'].dt.day_name()\n\n# ----------------------------\n# 1️⃣ Transaction / Revenue Trends\n# ----------------------------\n# 1. Monthly Revenue\nfig1 = px.line(df.groupby('year_month')['gross_revenue'].sum().reset_index(),\n               x='year_month', y='gross_revenue', title='Monthly Revenue', markers=True)\nfig1.show()\n\n# 2. Daily Revenue\nfig2 = px.line(df.groupby(df['timestamp'].dt.date)['gross_revenue'].sum().reset_index(),\n               x='timestamp', y='gross_revenue', title='Daily Revenue', markers=False)\nfig2.show()\n\n# 3. Weekday Revenue\nfig3 = px.bar(df.groupby('weekday')['gross_revenue'].sum().reindex(\n    [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]).reset_index(),\n               x='weekday', y='gross_revenue', title='Revenue by Weekday')\nfig3.show()\n\n# 4. Revenue by country\nfig4 = px.bar(df.groupby('country')['gross_revenue'].sum().reset_index(),\n              x='country', y='gross_revenue', title='Revenue by Country', color='country')\nfig4.show()\n\n# 5. Revenue by age\nfig5 = px.bar(df.groupby('age')['gross_revenue'].sum().reset_index(),\n              x='age', y='gross_revenue', title='Revenue by Age')\nfig5.show()\n\n# 6. Revenue by gender\nfig6 = px.bar(df.groupby('gender')['gross_revenue'].sum().reset_index(),\n              x='gender', y='gross_revenue', title='Revenue by Gender', color='gender')\nfig6.show()\n\n# 7. Revenue by loyalty tier\nfig7 = px.bar(df.groupby('loyalty_tier')['gross_revenue'].sum().reset_index(),\n              x='loyalty_tier', y='gross_revenue', title='Revenue by Loyalty Tier', color='loyalty_tier')\nfig7.show()\n\n# 8. Revenue by acquisition channel\nfig8 = px.bar(df.groupby('acquisition_channel')['gross_revenue'].sum().reset_index(),\n              x='acquisition_channel', y='gross_revenue', title='Revenue by Acquisition Channel', color='acquisition_channel')\nfig8.show()\n\n# 9. Revenue by product category\nfig9 = px.bar(df.groupby('category')['gross_revenue'].sum().reset_index(),\n              x='category', y='gross_revenue', title='Revenue by Category', color='category')\nfig9.show()\n\n# 10. Revenue by brand\nfig10 = px.bar(df.groupby('brand')['gross_revenue'].sum().reset_index(),\n               x='brand', y='gross_revenue', title='Revenue by Brand', color='brand')\nfig10.show()\n\n# ----------------------------\n# 2️⃣ Quantity / Discount Analysis\n# ----------------------------\n# 11. Quantity by category\nfig11 = px.bar(df.groupby('category')['quantity'].sum().reset_index(),\n               x='category', y='quantity', title='Total Quantity Sold by Category', color='category')\nfig11.show()\n\n# 12. Quantity by brand\nfig12 = px.bar(df.groupby('brand')['quantity'].sum().reset_index(),\n               x='brand', y='quantity', title='Quantity Sold by Brand', color='brand')\nfig12.show()\n\n# 13. Discount applied distribution\nfig13 = px.histogram(df, x='discount_applied', nbins=20, title='Discount Applied Distribution')\nfig13.show()\n\n# 14. Discount vs Revenue\nfig14 = px.scatter(df, x='discount_applied', y='gross_revenue', title='Discount vs Revenue', trendline='ols')\nfig14.show()\n\n# 15. Quantity vs Revenue\nfig15 = px.scatter(df, x='quantity', y='gross_revenue', title='Quantity vs Revenue', trendline='ols')\nfig15.show()\n\n# ----------------------------\n# 3️⃣ Campaign & Refund Analysis\n# ----------------------------\n# 16. Revenue by campaign\nfig16 = px.bar(df.groupby('campaign_id')['gross_revenue'].sum().reset_index(),\n               x='campaign_id', y='gross_revenue', title='Revenue by Campaign')\nfig16.show()\n\n# 17. Number of refunds\nfig17 = px.bar(df.groupby('refund_flag')['transaction_id'].count().reset_index(),\n               x='refund_flag', y='transaction_id', title='Number of Refunds')\nfig17.show()\n\n# 18. Refunds by country\nfig18 = px.bar(df[df['refund_flag']==1].groupby('country')['transaction_id'].count().reset_index(),\n               x='country', y='transaction_id', title='Refunds by Country')\nfig18.show()\n\n# 19. Refunds by category\nfig19 = px.bar(df[df['refund_flag']==1].groupby('category')['transaction_id'].count().reset_index(),\n               x='category', y='transaction_id', title='Refunds by Category')\nfig19.show()\n\n# 20. Refunds by brand\nfig20 = px.bar(df[df['refund_flag']==1].groupby('brand')['transaction_id'].count().reset_index(),\n               x='brand', y='transaction_id', title='Refunds by Brand')\nfig20.show()\n\n# ----------------------------\n# 4️⃣ Customer Signup Analysis\n# ----------------------------\n# 21. Customers by country\nfig21 = px.bar(df.groupby('country')['customer_id'].nunique().reset_index(),\n               x='country', y='customer_id', title='Number of Customers by Country')\nfig21.show()\n\n# 22. Customers by loyalty tier\nfig22 = px.bar(df.groupby('loyalty_tier')['customer_id'].nunique().reset_index(),\n               x='loyalty_tier', y='customer_id', title='Number of Customers by Loyalty Tier')\nfig22.show()\n\n# 23. Customers by acquisition channel\nfig23 = px.bar(df.groupby('acquisition_channel')['customer_id'].nunique().reset_index(),\n               x='acquisition_channel', y='customer_id', title='Customers by Acquisition Channel')\nfig23.show()\n\n# 24. Customers by age\nfig24 = px.histogram(df, x='age', nbins=20, title='Customer Age Distribution')\nfig24.show()\n\n# 25. Customers by gender\nfig25 = px.bar(df.groupby('gender')['customer_id'].nunique().reset_index(),\n               x='gender', y='customer_id', title='Customers by Gender', color='gender')\nfig25.show()\n\n# ----------------------------\n# 5️⃣ Product Launch & Price Analysis\n# ----------------------------\n# 26. Product base price distribution\nfig26 = px.histogram(df, x='base_price', nbins=50, title='Product Base Price Distribution')\nfig26.show()\n\n# 27. Revenue by is_premium\nfig27 = px.bar(df.groupby('is_premium')['gross_revenue'].sum().reset_index(),\n               x='is_premium', y='gross_revenue', title='Revenue by Premium Product')\nfig27.show()\n\n# 28. Revenue by launch year\ndf['launch_year'] = pd.to_datetime(df['launch_date'], errors='coerce').dt.year\nfig28 = px.bar(df.groupby('launch_year')['gross_revenue'].sum().reset_index(),\n               x='launch_year', y='gross_revenue', title='Revenue by Product Launch Year')\nfig28.show()\n\n# 29. Quantity sold by launch year\nfig29 = px.bar(df.groupby('launch_year')['quantity'].sum().reset_index(),\n               x='launch_year', y='quantity', title='Quantity Sold by Launch Year')\nfig29.show()\n\n# 30. Revenue by category & country\nfig30 = px.bar(df.groupby(['country','category'])['gross_revenue'].sum().reset_index(),\n               x='category', y='gross_revenue', color='country', title='Revenue by Category & Country')\nfig30.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:48:20.279348Z","iopub.execute_input":"2025-12-03T11:48:20.279683Z","iopub.status.idle":"2025-12-03T11:48:30.289059Z","shell.execute_reply.started":"2025-12-03T11:48:20.279656Z","shell.execute_reply":"2025-12-03T11:48:30.287696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\n\n# Ensure timestamp is datetime\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf['year_month'] = df['timestamp'].dt.to_period('M').astype(str)\ndf['weekday'] = df['timestamp'].dt.day_name()\ndf['launch_year'] = pd.to_datetime(df['launch_date'], errors='coerce').dt.year\n\n# ----------------------------\n# 31. Revenue by country & gender\n# ----------------------------\nfig31 = px.bar(df.groupby(['country','gender'])['gross_revenue'].sum().reset_index(),\n               x='country', y='gross_revenue', color='gender', barmode='group',\n               title='Revenue by Country & Gender')\nfig31.show()\n\n# ----------------------------\n# 32. Revenue by category & gender\n# ----------------------------\nfig32 = px.bar(df.groupby(['category','gender'])['gross_revenue'].sum().reset_index(),\n               x='category', y='gross_revenue', color='gender', barmode='group',\n               title='Revenue by Category & Gender')\nfig32.show()\n\n# ----------------------------\n# 33. Revenue by age groups\n# ----------------------------\n# Create age bins\nbins = [18, 25, 35, 45, 55, 65, 100]\nlabels = ['18-24','25-34','35-44','45-54','55-64','65+']\ndf['age_group'] = pd.cut(df['age'], bins=bins, labels=labels, right=False)\nage_group_rev = df.groupby('age_group')['gross_revenue'].sum().reset_index()\n\nfig33 = px.bar(age_group_rev, x='age_group', y='gross_revenue', title='Revenue by Age Group')\nfig33.show()\n\n# ----------------------------\n# 34. Revenue by weekday & category\n# ----------------------------\nfig34 = px.bar(df.groupby(['weekday','category'])['gross_revenue'].sum().reset_index(),\n               x='weekday', y='gross_revenue', color='category', barmode='group',\n               title='Revenue by Weekday & Category')\nfig34.show()\n\n# ----------------------------\n# 35. Quantity sold by country & category\n# ----------------------------\nfig35 = px.bar(df.groupby(['country','category'])['quantity'].sum().reset_index(),\n               x='category', y='quantity', color='country', barmode='group',\n               title='Quantity Sold by Country & Category')\nfig35.show()\n\n# ----------------------------\n# 36. Refunds by country & category\n# ----------------------------\nrefund_df = df[df['refund_flag']==1]\nfig36 = px.bar(refund_df.groupby(['country','category'])['transaction_id'].count().reset_index(),\n               x='category', y='transaction_id', color='country', barmode='group',\n               title='Refunds by Country & Category')\nfig36.show()\n\n# ----------------------------\n# 37. Discount applied by category\n# ----------------------------\nfig37 = px.box(df, x='category', y='discount_applied', title='Discount Distribution by Category')\nfig37.show()\n\n# ----------------------------\n# 38. Gross revenue vs quantity scatter\n# ----------------------------\nfig38 = px.scatter(df, x='quantity', y='gross_revenue', color='category',\n                   size='discount_applied', title='Revenue vs Quantity (Size=Discount)')\nfig38.show()\n\n# ----------------------------\n# 39. Revenue by launch year & category\n# ----------------------------\nfig39 = px.bar(df.groupby(['launch_year','category'])['gross_revenue'].sum().reset_index(),\n               x='launch_year', y='gross_revenue', color='category', barmode='group',\n               title='Revenue by Launch Year & Category')\nfig39.show()\n\n# ----------------------------\n# 40. Premium vs Non-Premium Revenue by Category\n# ----------------------------\nfig40 = px.bar(df.groupby(['category','is_premium'])['gross_revenue'].sum().reset_index(),\n               x='category', y='gross_revenue', color='is_premium', barmode='group',\n               title='Revenue by Category & Premium Status')\nfig40.show()\n\n# ----------------------------\n# 41. Top 10 brands by revenue\n# ----------------------------\ntop_brands = df.groupby('brand')['gross_revenue'].sum().sort_values(ascending=False).head(10).reset_index()\nfig41 = px.bar(top_brands, x='brand', y='gross_revenue', title='Top 10 Brands by Revenue')\nfig41.show()\n\n# ----------------------------\n# 42. Top 10 categories by quantity sold\n# ----------------------------\ntop_categories = df.groupby('category')['quantity'].sum().sort_values(ascending=False).head(10).reset_index()\nfig42 = px.bar(top_categories, x='category', y='quantity', title='Top 10 Categories by Quantity Sold')\nfig42.show()\n\n# ----------------------------\n# 43. Revenue trend for top 5 countries\n# ----------------------------\ntop_countries = df.groupby('country')['gross_revenue'].sum().sort_values(ascending=False).head(5).index\ntop_country_trend = df[df['country'].isin(top_countries)].groupby(['year_month','country'])['gross_revenue'].sum().reset_index()\n\nfig43 = px.line(top_country_trend, x='year_month', y='gross_revenue', color='country',\n                title='Revenue Trend for Top 5 Countries', markers=True)\nfig43.show()\n\n# ----------------------------\n# 44. Revenue trend for top 5 categories\n# ----------------------------\ntop_cats = df.groupby('category')['gross_revenue'].sum().sort_values(ascending=False).head(5).index\ntop_cat_trend = df[df['category'].isin(top_cats)].groupby(['year_month','category'])['gross_revenue'].sum().reset_index()\n\nfig44 = px.line(top_cat_trend, x='year_month', y='gross_revenue', color='category',\n                title='Revenue Trend for Top 5 Categories', markers=True)\nfig44.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:48:30.291369Z","iopub.execute_input":"2025-12-03T11:48:30.292187Z","iopub.status.idle":"2025-12-03T11:48:31.333008Z","shell.execute_reply.started":"2025-12-03T11:48:30.292160Z","shell.execute_reply":"2025-12-03T11:48:31.331343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Ensure timestamp is datetime\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf['year_month'] = df['timestamp'].dt.to_period('M').astype(str)\ndf['weekday'] = df['timestamp'].dt.day_name()\ndf['launch_year'] = pd.to_datetime(df['launch_date'], errors='coerce').dt.year\n\n# ----------------------------\n# 45. Heatmap: Revenue by Category & Month\n# ----------------------------\ncat_month_rev = df.groupby(['category','year_month'])['gross_revenue'].sum().reset_index()\nfig45 = px.density_heatmap(cat_month_rev, x='year_month', y='category', z='gross_revenue',\n                           color_continuous_scale='Viridis', title='Revenue Heatmap by Category & Month')\nfig45.show()\n\n# ----------------------------\n# 46. Heatmap: Revenue by Country & Month\n# ----------------------------\ncountry_month_rev = df.groupby(['country','year_month'])['gross_revenue'].sum().reset_index()\nfig46 = px.density_heatmap(country_month_rev, x='year_month', y='country', z='gross_revenue',\n                           color_continuous_scale='Cividis', title='Revenue Heatmap by Country & Month')\nfig46.show()\n\n# ----------------------------\n# 47. Treemap: Revenue by Country & Category\n# ----------------------------\ntreemap_rev = df.groupby(['country','category'])['gross_revenue'].sum().reset_index()\nfig47 = px.treemap(treemap_rev, path=['country','category'], values='gross_revenue',\n                   title='Revenue Distribution by Country & Category', color='gross_revenue',\n                   color_continuous_scale='RdBu')\nfig47.show()\n\n# ----------------------------\n# 48. Sunburst: Revenue by Category & Brand\n# ----------------------------\nsunburst_rev = df.groupby(['category','brand'])['gross_revenue'].sum().reset_index()\nfig48 = px.sunburst(sunburst_rev, path=['category','brand'], values='gross_revenue',\n                    title='Revenue Distribution by Category & Brand')\nfig48.show()\n\n# ----------------------------\n# 49. Scatter Matrix: Quantity, Revenue, Discount, Age\n# ----------------------------\nfig49 = px.scatter_matrix(df, dimensions=['quantity','gross_revenue','discount_applied','age'],\n                          color='category', title='Scatter Matrix: Revenue, Quantity, Discount, Age')\nfig49.show()\n\n# ----------------------------\n# 50. Bubble Chart: Revenue vs Quantity by Brand\n# ----------------------------\nbrand_stats = df.groupby('brand').agg({'gross_revenue':'sum','quantity':'sum','discount_applied':'mean'}).reset_index()\nfig50 = px.scatter(brand_stats, x='quantity', y='gross_revenue', size='discount_applied',\n                   color='brand', hover_name='brand', title='Revenue vs Quantity by Brand (Bubble=Discount)')\nfig50.show()\n\n# ----------------------------\n# 51. Stacked Area: Revenue by Category over Time\n# ----------------------------\narea_df = df.groupby(['year_month','category'])['gross_revenue'].sum().reset_index()\nfig51 = px.area(area_df, x='year_month', y='gross_revenue', color='category',\n                title='Revenue by Category Over Time')\nfig51.show()\n\n# ----------------------------\n# 52. Line + Markers: Revenue Trend Top 5 Countries\n# ----------------------------\ntop_countries = df.groupby('country')['gross_revenue'].sum().sort_values(ascending=False).head(5).index\ntop_country_df = df[df['country'].isin(top_countries)].groupby(['year_month','country'])['gross_revenue'].sum().reset_index()\nfig52 = px.line(top_country_df, x='year_month', y='gross_revenue', color='country', markers=True,\n                title='Revenue Trend for Top 5 Countries')\nfig52.show()\n\n# ----------------------------\n# 53. Box Plot: Revenue Distribution by Loyalty Tier\n# ----------------------------\nfig53 = px.box(df, x='loyalty_tier', y='gross_revenue', color='loyalty_tier',\n               title='Revenue Distribution by Loyalty Tier')\nfig53.show()\n\n# ----------------------------\n# 54. Violin Plot: Quantity Distribution by Category\n# ----------------------------\nfig54 = px.violin(df, x='category', y='quantity', color='category', box=True, points='all',\n                  title='Quantity Distribution by Category')\nfig54.show()\n\n# ----------------------------\n# 55. Correlation Heatmap (Numeric Features)\n# ----------------------------\nimport plotly.figure_factory as ff\nnumeric_cols = ['quantity','discount_applied','gross_revenue','age','base_price','is_premium']\ncorr_matrix = df[numeric_cols].corr().round(2)\nfig55 = ff.create_annotated_heatmap(z=corr_matrix.values, x=list(corr_matrix.columns), y=list(corr_matrix.index),\n                                    colorscale='Viridis', showscale=True)\nfig55.update_layout(title='Correlation Heatmap (Numeric Features)')\nfig55.show()\n\n# ----------------------------\n# 56. Time Series Decomposition (Optional, requires statsmodels)\n# ----------------------------\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nts = df.groupby('timestamp')['gross_revenue'].sum().asfreq('D').fillna(0)\nresult = seasonal_decompose(ts, model='additive', period=30)\nfig56 = go.Figure()\nfig56.add_trace(go.Scatter(y=result.trend, name='Trend'))\nfig56.add_trace(go.Scatter(y=result.seasonal, name='Seasonality'))\nfig56.add_trace(go.Scatter(y=result.resid, name='Residual'))\nfig56.update_layout(title='Revenue Time Series Decomposition')\nfig56.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:48:31.335535Z","iopub.execute_input":"2025-12-03T11:48:31.335838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n\n# -----------------------------\n# Load & Merge datasets\n# -----------------------------\npath = \"/kaggle/input/marketing-and-e-commerce-analytics-dataset/\"\n\nproducts = pd.read_csv(path + \"products.csv\")\ncustomers = pd.read_csv(path + \"customers.csv\")\ntransactions = pd.read_csv(path + \"transactions.csv\")\n\ndf = transactions.merge(customers, on=\"customer_id\", how=\"left\")\ndf = df.merge(products, on=\"product_id\", how=\"left\")\n\n# -----------------------------\n# Ensure datetime\n# -----------------------------\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf['signup_date'] = pd.to_datetime(df['signup_date'])\ndf['launch_date'] = pd.to_datetime(df['launch_date'], errors='coerce')\n\n# -----------------------------\n# Handle Missing Values\n# -----------------------------\nnumeric_cols = ['quantity','discount_applied','age','base_price','gross_revenue']\nfor col in numeric_cols:\n    df[col] = df[col].fillna(df[col].median())\n\ncategorical_cols = ['country','gender','loyalty_tier','acquisition_channel','category','brand','is_premium']\nfor col in categorical_cols:\n    df[col] = df[col].fillna(\"Unknown\")\n\n# -----------------------------\n# Feature Engineering\n# -----------------------------\n# Temporal Features\ndf['year'] = df['timestamp'].dt.year\ndf['month'] = df['timestamp'].dt.month\ndf['day'] = df['timestamp'].dt.day\ndf['weekday'] = df['timestamp'].dt.weekday\ndf['hour'] = df['timestamp'].dt.hour\n\n# Product & customer features\ndf['product_age_days'] = (df['timestamp'] - df['launch_date']).dt.days\ndf['product_age_days'] = df['product_age_days'].fillna(df['product_age_days'].median())\n\ndf['days_since_signup'] = (df['timestamp'] - df['signup_date']).dt.days\ndf['days_since_signup'] = df['days_since_signup'].fillna(df['days_since_signup'].median())\n\n# Lag Features: sort by customer\ndf = df.sort_values(by=['customer_id','timestamp'])\n\ndf['customer_prev_revenue'] = df.groupby('customer_id')['gross_revenue'].shift(1).fillna(0)\ndf['customer_cum_revenue'] = df.groupby('customer_id')['gross_revenue'].cumsum() - df['gross_revenue']\ndf['customer_prev_quantity'] = df.groupby('customer_id')['quantity'].shift(1).fillna(0)\n\n# Encode Categorical Features\nfor col in categorical_cols:\n    le = LabelEncoder()\n    df[col] = le.fit_transform(df[col].astype(str))\n\n# -----------------------------\n# Features and Target\n# -----------------------------\ntarget = 'gross_revenue'\nfeatures = [\n    'quantity','discount_applied','age','days_since_signup','product_age_days',\n    'customer_prev_revenue','customer_cum_revenue','customer_prev_quantity',\n    'year','month','day','weekday','hour'\n] + categorical_cols\n\nX = df[features]\ny = df[target]\n\n# -----------------------------\n# Train-Validation Split\n# -----------------------------\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# -----------------------------\n# LightGBM Dataset\n# -----------------------------\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n\n# -----------------------------\n# LightGBM Parameters\n# -----------------------------\nparams = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.05,\n    'num_leaves': 131,\n    'max_depth': -1,\n    'min_data_in_leaf': 20,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': -1\n}\n\n# -----------------------------\n# Train LightGBM Model\n# -----------------------------\nmodel = lgb.train(\n    params,\n    lgb_train,\n    num_boost_round=1000,\n    valid_sets=[lgb_train, lgb_val],\n)\n\n# -----------------------------\n# Predictions\n# -----------------------------\ny_train_pred = model.predict(X_train, num_iteration=model.best_iteration)\ny_val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n\n# -----------------------------\n# Metrics\n# -----------------------------\ndef print_metrics(y_true, y_pred, dataset_name):\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = mean_squared_error(y_true, y_pred, squared=False)\n    r2 = r2_score(y_true, y_pred)\n    print(f\"\\n{dataset_name} Metrics:\")\n    print(f\"MAE: {mae:.2f}\")\n    print(f\"RMSE: {rmse:.2f}\")\n    print(f\"R^2: {r2:.2f}\")\n\nprint_metrics(y_train, y_train_pred, \"Train\")\nprint_metrics(y_val, y_val_pred, \"Validation\")\n\n# -----------------------------\n# Feature Importance\n# -----------------------------\nlgb.plot_importance(model, max_num_features=20, importance_type='gain', figsize=(10,6))\nplt.title(\"Top 20 Feature Importance\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:50:50.482022Z","iopub.execute_input":"2025-12-03T11:50:50.483811Z","iopub.status.idle":"2025-12-03T11:51:08.058518Z","shell.execute_reply.started":"2025-12-03T11:50:50.483690Z","shell.execute_reply":"2025-12-03T11:51:08.056456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom tensorflow.keras.layers import Dense, Dropout\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T12:05:04.260595Z","iopub.execute_input":"2025-12-03T12:05:04.260849Z","iopub.status.idle":"2025-12-03T12:05:04.269003Z","shell.execute_reply.started":"2025-12-03T12:05:04.260835Z","shell.execute_reply":"2025-12-03T12:05:04.267597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# -----------------------------\n# Load & Merge datasets\n# -----------------------------\n\n# -----------------------------\n# Train Linear Regression\n# -----------------------------\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\n\n# -----------------------------\n# Predictions\n# -----------------------------\ny_train_pred = lr_model.predict(X_train)\ny_val_pred = lr_model.predict(X_val)\n\n# -----------------------------\n# Metrics\n# -----------------------------\ndef print_metrics(y_true, y_pred, dataset_name):\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = mean_squared_error(y_true, y_pred, squared=False)\n    r2 = r2_score(y_true, y_pred)\n    print(f\"\\n{dataset_name} Metrics:\")\n    print(f\"MAE: {mae:.2f}\")\n    print(f\"RMSE: {rmse:.2f}\")\n    print(f\"R^2: {r2:.2f}\")\n\nprint_metrics(y_train, y_train_pred, \"Train\")\nprint_metrics(y_val, y_val_pred, \"Validation\")\n\n# -----------------------------\n# Feature Importance (Coefficients)\n# -----------------------------\ncoefs = pd.Series(lr_model.coef_, index=X_train.columns)\ncoefs = coefs.sort_values(key=abs, ascending=False)[:20]\n\nplt.figure(figsize=(10,6))\ncoefs.plot(kind='barh')\nplt.title(\"Top 20 Feature Coefficients (Linear Regression)\")\nplt.gca().invert_yaxis()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:51:08.069156Z","iopub.execute_input":"2025-12-03T11:51:08.069417Z","iopub.status.idle":"2025-12-03T11:51:08.385867Z","shell.execute_reply.started":"2025-12-03T11:51:08.069401Z","shell.execute_reply":"2025-12-03T11:51:08.383717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\n\n\n# -----------------------------\n# Load & Merge datasets\n# -----------------------------\n# -----------------------------\n# Build ANN Model\n# -----------------------------\nmodel = Sequential([\n    Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(1, activation='linear')  # Regression output\n])\n\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\n# -----------------------------\n# Train ANN\n# -----------------------------\nhistory = model.fit(\n    X_train_scaled, y_train,\n    validation_data=(X_val_scaled, y_val),\n    epochs=50,\n    batch_size=512,\n    verbose=1\n)\n\n# -----------------------------\n# Predictions\n# -----------------------------\ny_train_pred = model.predict(X_train_scaled).flatten()\ny_val_pred = model.predict(X_val_scaled).flatten()\n\n# -----------------------------\n# Metrics\n# -----------------------------\ndef print_metrics(y_true, y_pred, dataset_name):\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = mean_squared_error(y_true, y_pred, squared=False)\n    r2 = r2_score(y_true, y_pred)\n    print(f\"\\n{dataset_name} Metrics:\")\n    print(f\"MAE: {mae:.2f}\")\n    print(f\"RMSE: {rmse:.2f}\")\n    print(f\"R^2: {r2:.2f}\")\n\nprint_metrics(y_train, y_train_pred, \"Train\")\nprint_metrics(y_val, y_val_pred, \"Validation\")\n\n# -----------------------------\n# Plot Training Loss\n# -----------------------------\nplt.figure(figsize=(10,5))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('MSE Loss')\nplt.title('Training vs Validation Loss')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T12:05:09.287006Z","iopub.execute_input":"2025-12-03T12:05:09.287391Z","iopub.status.idle":"2025-12-03T12:05:09.320040Z","shell.execute_reply.started":"2025-12-03T12:05:09.287373Z","shell.execute_reply":"2025-12-03T12:05:09.318293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}